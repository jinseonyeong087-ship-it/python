# 1ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
from matplotlib import pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import plot_tree as sk_plot_tree
from xgboost import XGBClassifier, plot_tree as xgb_plot_tree

# 2ï¸âƒ£ í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rc('font', family='Malgun Gothic')
plt.rcParams['axes.unicode_minus'] = False

# 3ï¸âƒ£ ë°ì´í„° ë¡œë“œ
iris = load_iris()
X = iris.data
y = iris.target

# 4ï¸âƒ£ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5ï¸âƒ£ ëª¨ë¸ ìƒì„±
forest = RandomForestClassifier(n_estimators=3, random_state=42)
xg = XGBClassifier(
    n_estimators=3,      # íŠ¸ë¦¬ 3ê°œë§Œ (ë³´ê¸° ì‰½ê²Œ)
    learning_rate=0.1,
    max_depth=3,
    random_state=42,
    eval_metric='mlogloss'
)

# 6ï¸âƒ£ í•™ìŠµ
forest.fit(X_train, y_train)
xg.fit(X_train, y_train)

# 7ï¸âƒ£ ëœë¤í¬ë ˆìŠ¤íŠ¸ íŠ¸ë¦¬ ì‹œê°í™”
plt.figure(figsize=(20, 8))
for i, estimator in enumerate(forest.estimators_):
    plt.subplot(1, len(forest.estimators_), i + 1)
    sk_plot_tree(
        estimator,
        feature_names=iris.feature_names,
        class_names=list(iris.target_names),
        filled=True,
        fontsize=8
    )
    plt.title(f"ëœë¤í¬ë ˆìŠ¤íŠ¸ íŠ¸ë¦¬ {i + 1}")

plt.suptitle("ëœë¤í¬ë ˆìŠ¤íŠ¸ êµ¬ì„± íŠ¸ë¦¬ ì‹œê°í™”", fontsize=16)
plt.tight_layout()
plt.show()

# 8ï¸âƒ£ XGBoost íŠ¸ë¦¬ ì‹œê°í™”
plt.figure(figsize=(25, 10))
for i in range(3):  # 0~2ë²ˆ íŠ¸ë¦¬ë§Œ ì‹œê°í™”
    plt.subplot(1, 3, i + 1)
    xgb_plot_tree(xg, num_trees=i, rankdir='LR')  # ğŸ‘ˆ XGBoost ì „ìš© í•¨ìˆ˜
    plt.title(f"XGBoost íŠ¸ë¦¬ {i+1}")

plt.suptitle("XGBoost íŠ¸ë¦¬ ì‹œê°í™”", fontsize=18, y=1.02)
plt.tight_layout()
plt.show()
